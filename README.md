# Responsible Data Workshop Resources
This repository contains resources that are relevant to our workshop series on responsibly training foundation models.
If you are interested in contributing resources to this repository, please submit a Pull Request. 

### Upcoming Workshops
- [September 15th-16th @ ASU California Center](https://responsible-data-workshop.github.io/la2025/)
- [October 18th or 19th @ CSCW 2025](https://responsible-data-workshop.github.io/cscw2025/)


## Resources
### Considerations and Frameworks
- [A Taxonomy of Challenges to Curating Fair Datasets](https://proceedings.neurips.cc/paper_files/paper/2024/hash/b142e78db191e19b17e60c1425a28b52-Abstract-Datasets_and_Benchmarks_Track.html)
- [Ethical Considerations for Responsible Data Curation](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad3ebc951f43d1e9ed20187a7b5bc4ee-Abstract-Datasets_and_Benchmarks.html)
- 

### Background on Current Dataset Curation Practices
### Overview
- [A Survey on Data Selection for Language Models](https://arxiv.org/abs/2402.16827)
- [Data Curation for LLMs](https://dcai.csail.mit.edu/2024/data-curation-llms/)

### Pre-Training Data
- [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159)
### Preference Data
- [Reinforcement Learning from Human Feedback](https://rlhfbook.com/)
   - [Section on Preference Data](https://rlhfbook.com/c/06-preference-data.html)
   - [Section on Constitutional AI & AI Feedback](https://rlhfbook.com/c/13-cai.html)
- [The Future of Open Human Feedback](https://arxiv.org/abs/2408.16961)
- [The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models](https://arxiv.org/abs/2404.16019)
